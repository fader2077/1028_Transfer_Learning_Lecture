{"cells":[{"cell_type":"markdown","metadata":{"id":"cyPUYLH-0KTO"},"source":["# **遷移式學習** -站在巨人的肩膀上的深度學習"]},{"cell_type":"markdown","metadata":{"id":"1Pst-jvl0nHB"},"source":["## 1. 人工智慧的問題解決方法: CRISP-DM 框架\n","<img src=\"https://www.tribloom.com/wp-content/uploads/2019/08/CRISP-DM_Process_Diagram-768x769.png\" height=500>\n","\n","Follow the CRSIP-DM method\n","1. Step 1: Import library, import data\n","2. Step 2: Pre-processing (missing data, categorical type, normalization, format transform)\n","3. Step 3: Build ML Model\n","4. Step 4: Evaluate Model\n","5. Step 5: Deploy (Prediction)"]},{"cell_type":"markdown","metadata":{"id":"yxGYOxNX1XcM"},"source":["# Step 1: Import library and Loading Data"]},{"cell_type":"markdown","metadata":{"id":"SfrE88_O1k7l"},"source":["## Step 1.1 install package "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gblCHgmeq46l"},"outputs":[],"source":["!pip3 install torch torchvision\n","!pip3 install gradio\n","!pip install Pillow\n"]},{"cell_type":"markdown","metadata":{"id":"GKoAojtk1yEr"},"source":["## Step 1-2: 匯入套件\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0BgsFW_rK5Z"},"outputs":[],"source":["import gradio as gr\n","from torchvision import datasets, transforms, models\n","import torch\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"jSMfIuik4EQ-"},"source":["## Step 1-3: 使用GPU on Colab\n","* device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","* model.to(device)\n","* inputs = inputs.to(device)\n","* labels = labels.to(device)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlUmKI-nrYcD"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"XWRIq90C7apl"},"source":[]},{"cell_type":"markdown","metadata":{"id":"mmRDQeF54yvO"},"source":["## Step 1-4 Loading data from github with folders as labels\n","\n","* ant and bee github <br>\n","!git clone https://github.com/jaddoescad/ants_and_bees.git\n","\n","* medical mask github <br>\n","!git clone https://github.com/chandrikadeb7/Face-Mask-Detection.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vN6k2JLOrY7Q"},"outputs":[],"source":["!git clone https://github.com/chandrikadeb7/Face-Mask-Detection.git"]},{"cell_type":"markdown","metadata":{"id":"QFgQMvmQ72-c"},"source":["## Step 1-5 確認目錄\n","* with_mask 是 Label =0\n","* without_mask 是 Label=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9qqe5cb-rfVE"},"outputs":[],"source":["!ls ./Face-Mask-Detection/dataset/"]},{"cell_type":"markdown","metadata":{"id":"dO7Cf0Qo8KAn"},"source":["# Step 2: Data Preprocessing\n"]},{"cell_type":"markdown","metadata":{"id":"PdU8qbt6_CQu"},"source":["## Step 2-1 資料格式轉換\n","* 將資料做一些normalization 以增強深度學習模型效能\n","* 將資料做一些格式轉換以符合深度學習模型的輸入格式 Tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUzaCfSPrrUM"},"outputs":[],"source":["transform_train = transforms.Compose([transforms.Resize((224,224)),\n","                                      transforms.RandomHorizontalFlip(),\n","                                      transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n","                                      transforms.ColorJitter(brightness=1, contrast=1, saturation=1),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","                               ])\n","\n","\n","transform = transforms.Compose([transforms.Resize((224,224)),\n","                               transforms.ToTensor(),\n","                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","                               ])\n","\n","training_dataset = datasets.ImageFolder('Face-Mask-Detection/dataset/', transform=transform_train)\n","validation_dataset = datasets.ImageFolder('Face-Mask-Detection/dataset/', transform=transform)\n","\n","training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=20, shuffle=True)\n","validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size = 20, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZOSN1NU-dXT"},"outputs":[],"source":["validation_dataset = datasets.ImageFolder('Face-Mask-Detection/dataset/', transform=transform)\n","validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size = 20, shuffle=False)\n","\n","print(len(training_dataset))\n","print(len(validation_dataset))"]},{"cell_type":"markdown","metadata":{"id":"sGz1qMef-6lY"},"source":["## Step 2-2 觀察一下影像資料"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"80BwGdVk-5p6"},"outputs":[],"source":["def im_convert(tensor):\n","  image = tensor.cpu().clone().detach().numpy()\n","  image = image.transpose(1, 2, 0)\n","  image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n","  image = image.clip(0, 1)\n","  return image\n","\n","classes=('mask', 'no_mask')\n","\n","dataiter = iter(training_loader)\n","images,labels = dataiter.next()\n","fig = plt.figure(figsize=(25, 4))\n","\n","for idx in np.arange(20):\n","  ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n","  plt.imshow(im_convert(images[idx]))\n","  ax.set_title(classes[labels[idx].item()])"]},{"cell_type":"markdown","metadata":{"id":"q35SYV1T8znK"},"source":["# Step 3: Build Model\n","*使用pretraind vgg16模型"]},{"cell_type":"markdown","metadata":{"id":"vCpyA0Ao_5oj"},"source":["## Step 3-1: loading torchvision的 models 預先triain好的模型\n","* 所有pre-trained models in torchvision\n","\n","https://pytorch.org/vision/0.8/models.html\n","\n","* AlexNet\n","* VGG\n","* ResNet\n","* SqueezeNet\n","* DenseNet\n","* Inception v3\n","* GoogLeNet\n","* ShuffleNet v2\n","* MobileNet v2\n","* ResNeXt\n","* Wide ResNet\n","* MNASNet"]},{"cell_type":"markdown","metadata":{"id":"URIGhhX6CieV"},"source":["## Step 3-2 各經典模型summary <br>\n","https://ithelp.ithome.com.tw/articles/10192162\n","\n","<img src=\"https://ithelp.ithome.com.tw/upload/images/20171206/200019764r3qCPSJxX.png\" height=800, width=300>Vgg16</img>\n","\n","圖. VGG16 結構圖，圖片來源：Building powerful image classification models using very little data\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rI9W8CYcFava"},"source":[]},{"cell_type":"markdown","metadata":{"id":"-Y0PZBjvFfe-"},"source":["## Step 3-3  使用vgg16 pretrain 模型"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VxyjHyiWFYbL"},"outputs":[],"source":["model = models.vgg16(pretrained=True)"]},{"cell_type":"markdown","metadata":{"id":"1387If9vFkdf"},"source":["## Step 3-4 把梯度更新全部關閉, 只留後面三級分類層 (vgg16/vgg19示意圖)\n"," \n","<img src=\"https://ithelp.ithome.com.tw/upload/images/20171206/20001976yeCo1PvEOs.jpg\" width=500 />"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOqFtMIzrwXD"},"outputs":[],"source":["\n","for param in model.features.parameters():\n","  param.requires_grad = False\n","\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"M7pBiYA-Gcf6"},"source":["## Step 3-5 修改模型讓最後輸出兩類"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UMqAU1LgrxLp"},"outputs":[],"source":["import torch.nn as nn\n","n_inputs = model.classifier[6].in_features\n","last_layer = nn.Linear(n_inputs, len(classes)) # 2 classes 最後一級\n","model.classifier[6] = last_layer\n","model.to(device)\n","print(model)"]},{"cell_type":"markdown","source":["## Step 3-7 Training Model"],"metadata":{"id":"fdZqjYizJbGC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXana_nZsA1A"},"outputs":[],"source":["criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n","\n","epochs = 3\n","running_loss_history = []\n","running_corrects_history = []\n","val_running_loss_history = []\n","val_running_corrects_history = []\n","\n","for e in range(epochs):\n","  \n","  running_loss = 0.0\n","  running_corrects = 0.0\n","  val_running_loss = 0.0\n","  val_running_corrects = 0.0\n","  \n","  for inputs,labels in training_loader:\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","    outputs = model(inputs)\n","    loss = criterion(outputs, labels)\n","    \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    \n","    _, preds = torch.max(outputs, 1)\n","    running_loss += loss.item()\n","    running_corrects += torch.sum(preds == labels.data)\n","\n","  else:\n","    with torch.no_grad():\n","      for val_inputs, val_labels in validation_loader:\n","        val_inputs = val_inputs.to(device)\n","        val_labels = val_labels.to(device)\n","        val_outputs = model(val_inputs)\n","        val_loss = criterion(val_outputs, val_labels)\n","        \n","        _, val_preds = torch.max(val_outputs, 1)\n","        val_running_loss += val_loss.item()\n","        val_running_corrects += torch.sum(val_preds == val_labels.data)\n","      \n","    epoch_loss = running_loss/len(training_loader.dataset)\n","    epoch_acc = running_corrects.float()/ len(training_loader.dataset)\n","    running_loss_history.append(epoch_loss)\n","    running_corrects_history.append(epoch_acc.item())\n","    \n","    val_epoch_loss = val_running_loss/len(validation_loader.dataset)\n","    val_epoch_acc = val_running_corrects.float()/ len(validation_loader.dataset)\n","    val_running_loss_history.append(val_epoch_loss)\n","    val_running_corrects_history.append(val_epoch_acc.item())\n","    print('epoch :', (e+1))\n","    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n","    print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_epoch_loss, val_epoch_acc.item()))"]},{"cell_type":"markdown","source":[":# Step 4: Evaluate Model and See Results"],"metadata":{"id":"Hec4o239Jgfj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPf-Sz6wEsm3"},"outputs":[],"source":["plt.plot(running_loss_history, label='training loss')\n","plt.plot(val_running_loss_history, label='validation loss')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLsr3VqzEwcf"},"outputs":[],"source":["plt.plot(running_corrects_history, label='training accuracy')\n","plt.plot(val_running_corrects_history, label='validation accuracy')\n","plt.legend()"]},{"cell_type":"markdown","source":["# Step 5: Deploy Model for Practical Use\n","# Make an AI App"],"metadata":{"id":"FNTiuDPEJnud"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uDF91r_iPd8P"},"outputs":[],"source":["# !pip3 install pillow==4.0.0\n","import PIL.ImageOps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMgkQD5uEyq7"},"outputs":[],"source":["\n","\n","import requests\n","from PIL import Image\n","\n","url = 'http://media.rojaklah.com/wp-content/uploads/2017/09/19150232/1909bigstar1.jpg'\n","\n","response = requests.get(url, stream = True)\n","img = Image.open(response.raw)\n","plt.imshow(img)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HHGToXzAE6LN"},"outputs":[],"source":["\n","img = transform(img) \n","plt.imshow(im_convert(img))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGEAPiLGE_Ce"},"outputs":[],"source":["image = img.to(device).unsqueeze(0)\n","output = model(image)\n","_, pred = torch.max(output, 1)\n","print(classes[pred.item()])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FrmzbZaUFDDR"},"outputs":[],"source":["import requests\n","from PIL import Image\n","\n","url2='http://i1.kknews.cc/L4QL2XatiKQt9Z6yC-ttG4KM-Gk48BY/0.jpg'\n","response = requests.get(url2, stream = True)\n","img = Image.open(response.raw)\n","plt.imshow(img)\n","img = transform(img)\n","plt.imshow(im_convert(img))\n","image = img.to(device).unsqueeze(0)\n","output = model(image)\n","_, pred = torch.max(output, 1)\n","print(classes[pred.item()])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GMT1pCxFGVn"},"outputs":[],"source":["print(classes[pred.item()])\n"]},{"cell_type":"markdown","metadata":{"id":"IA_s68872cFk"},"source":["## 特別介紹 gradio :https://www.gradio.app/ <br>\n","Gradio is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it, anywhere!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u3Ku0ZcTsEX6"},"outputs":[],"source":["def predict(img):\n","    labels = ['mask', 'no_mask']\n","    img = transform(img)\n","    image = img.to(device).unsqueeze(0)\n","    output = model(image)\n","    _, pred = torch.max(output, 1)\n","    return labels[pred.item()]\n","\n","gr.Interface(fn=predict, \n","             inputs=gr.Image(type=\"pil\"),\n","             outputs='label').launch(debug=True, show_error=True)"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"private_outputs":true},"kernelspec":{"display_name":"Python 3.10.4 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"61b1abc921a251ea01e5488be3aa0223db5bae4de73e56a819c48c7237848b4b"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}